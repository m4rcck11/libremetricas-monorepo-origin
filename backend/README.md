# Plataforma Altmetria - Backend API (v0.0.2)

API REST de alta performance desenvolvida para fornecer m√©tricas altm√©tricas de publica√ß√µes acad√™micas da Am√©rica Latina. O sistema utiliza uma arquitetura **OLAP (Online Analytical Processing)** baseada em DuckDB e arquivos Parquet, garantindo respostas r√°pidas com baixo custo computacional.

## Tecnologias

- **Runtime:** Python 3.11 (ou superior).
- **Framework Web:** FastAPI
- **Engine Anal√≠tica:** DuckDB (Zero-copy sobre Parquet)
- **Servidor de Aplica√ß√£o:** Gunicorn + Uvicorn (Production Grade)
- **Seguran√ßa & Performance:** - SlowAPI (para Rate Limiting)
  - Pydantic 
  - Cachetools (Cache em mem√≥ria L1)

## üèóÔ∏è Arquitetura

O sistema faz um mapeamento din√¢mico dos arquivos na inicializa√ß√£o. Se o volume de arquivos crescer muito (10k+), podemos otimizar, mas para o volume atual √© instant√¢neo. Cold Start de at√© 5 segundos, queries em milissegundos. 

O projeto segue uma arquitetura segregada para garantir estabilidade em ambiente governamental/institucional:

1.  **API (Stateless):** Respons√°vel apenas pela leitura e agrega√ß√£o dos dados. N√£o realiza grava√ß√µes no banco principal em tempo de execu√ß√£o.
2.  **Dados (Persist√™ncia):** Os dados residem em arquivos `.parquet` e um cat√°logo DuckDB montados via Volume.
3.  **Ferramentas (ETL):** Scripts de coleta e processamento (`tools/`), atualmente desacoplados da execu√ß√£o da API.


## Executar localmente:

**Pr√©-requisitos**:
- Docker e Docker-Compose
- Python 3.11^

**Via Docker**

Configure as vari√°veis de ambiente
> O projeto inclui um .env.example. Voc√™ pode copi√°-lo e configur√°-lo manualmente ou us√°-lo para a configura√ß√£o no Kubernetes. 

**Prepare os Dados**: Coloque os arquivos .parquet e o banco analytics.duckdb  na pasta ./data local. 

4. Executar:
> docker compose up --build

Pronto! A API j√° est√° dispon√≠vel em http://localhost:8000


# Deploy em produ√ß√£o (Local/Cloud)

A aplica√ß√£o √© container first. 

1. Vari√°veis de ambiente segregadas (.env)

O container precisa das seguintes vari√°veis de ambiente:

> DATA_DIR -----> Caminho absoluto dentro do container -----> /app/data (default)
> DUCKDB_PATH -----> Caminho do arquivo de banco ------> /app/data/analytics.duckdb
> CORS_ORIGINS --> Configura√ß√µes de dom√≠nio (como n√£o sei, tudo est√° liberado) -> siteoficial.com.bre
> WORKERS ------> N√∫mero de processos em paralelo no gunicorn ---> 4 (default)

## Persist√™ncia dos dados

A pasta /app/data dentro do container precisa ser um volume montado com arquivos .parquet. A api n√£o popula esses dados sozinha. A atualiza√ß√£o precisa ser feita em jobs agendados que escrevem no mesmo volume.

**Para isso, temos scripts de apoio**

Em scripts/ inclu√≠ utilit√°rios de refer√™ncia para ambientes Linux/Debian, testados na minha m√°quina. Os shelss devem rodar em qualquer servi√ßo em servidores at√© bare-metal.

- setup-firewall.sh -> configura√ß√£o de firewall b√°sica (rever com infra do IBICT)
- setup-ssl.sh (automa√ß√£o de certbot para certificado ssl)
- deploy.sh (exemplo de esteira local)

## Endpoints principais

Os endpoints principais est√£o na documenta√ß√£o swagger em /docs. Seguimos o contrato estabelecido pelo frontend. 

### Sistema
- GET /health - Status da API e conex√£o com o 'Banco de Dados'
### M√©tricas e Agrega√ß√µes
- GET /events_sources - Eventos por fonte
- GET /events_years - Distribui√ß√£o por anos
- GET /fields_events - Eventos por √°rea de conhecimen to (OpenAlex)

### Busca
- POST /search_dois - Recupera√ß√£o de M√©tricas. Terminamos a implementa√ß√£o no frontend.

### Exporta√ß√£o

- **PRECISA DE CORRE√á√ÉO** - Extra√ß√£o de dados brutos com rate limiting restritivo. A inten√ß√£o √© modificar para extrair dados brutos, e n√£o apenas os dispon√≠veis no frontend. 



## Seguran√ßa da API 

#### Rate Limiting (configur√°vel na env)
#### Read-Only Database: Conex√£o com o DuckDB √© aberta estritamente em modo leitura (read_only=True), previne corrup√ß√£o de dados por concorr√™ncia.
#### Privil√©gios: o container roda como usu√°rio (sem root).

## Manuten√ß√£o e Atualiza√ß√£o dos dados

A pasta tools/ cont√©m scripts para coleta de novas m√©tricas oriundas do CrossRef, Bluesky e BORI 

-> Arquivos CrossRef dispon√≠veis em: (Arquivos pesados, servidor lento)
-> Arquivos Bluesky e Bori dispon√≠veis em: "" ---> No alibaba Cloud (Bucket P√∫blico) (Arquivos leves)
-> Arquivos OpenAlex dispon√≠veis em: "" --> No Google Cloud (Bucket P√∫blico) (Dados gigantes, servidor "r√°pido")

### Os scripts tem tratamento de erro, retry e os dados s√£o salvos incrementalmente para contornar eventuais falhas de rede.


**Nota**: Estes scripts devem ser executados em um processo separado (Worker ou CronJob) e n√£o no container da API, para evitar degrada√ß√£o de performance. 



## Inser√ß√£o dos Dados para An√°lise via DuckDB (em casos de atualiza√ß√£o)

Com o DuckDB temos um banco de dados de 12KB. Com o DuckDB, separamos a l√≥gica do banco de dados, que j√° est√° dividido em parquets. O arquivo >analyitics.duckdb √© apenas o c√≥digo.


### O que √© OLAP no nosso caso?

OLAP √© o porcessamento anal√≠tico online, diferente de OLTP (transacional).

Com o duckDB (engine vetorizada): com o python puro podemos ler uma linha, processar, ler outra e assim sucetivamente. Agora, com o DuckDB, o processo √©: ele l√™ todos os √≠tens de uma coluna e processa em vetores com instru√ß√µes da CPU, isso faz com que ele entregue os arquivos mais digeridos em poucos milissegundos. 

### Qual √© a arquitetura de dados?

Com DuckDB, a arquitetura de dados √© uma Lakehouse moderna. O banco de dados √© apenas um motor de processamento (DuckDB) que apenas l√™ os dados.

1. Arquitetura Zero-C√≥pia: O banco n√£o copia os dados para dentro dele. Tudo √© lido diretamente dos parquets. 
2. Computa√ß√£o sem estado (Stateless): Como o  arquivo de banco (.duckdb) guarda s√≥ os metadados, ele √© leve (12KB) e descart√°vel. Se o servidor parar de funcionar, os dados s√£o mantidos porque tudo est√° dentro dos arquivos parquet, esses, imut√°veis e baixados de fontes externas.
3. Performance OLAP: O DuckDB surgiu nas trends mais modernas entre desenvolvedores por usar execu√ß√£o vetorizada e sua inclina√ß√£o de uso para leitura de dados de IA. O DuckDB consulta parquets em milissegundos e nos isenta de estruturar SQL transacional, o que seria um exagero para apenas visualizar dados. Ex: ele l√™ apenas a coluna 'Ano' do arquivo parquet e ignora o resto.

### Exemplo de funcionamento

O duckdb usa √≠ndices impl√≠citos para fazer o alinhamento posicional. Veja essa consulta:

> ID, Titulo, Ano, Autor
> 1, "A Cura do C√¢ncer", 2023, "Dr. Silva"  <-- O computador l√™ a linha inteira
> 2, "Estudo de IA", 2022, "Dra. Santos"

Se voc√™ quer saber, por ex, quantos artigos s√£o de 2023, o computador l√™ a linha inteira do artigo em quest√£o (neste exemplo, o artigo A) joga fora o que n√£o precisa e guarda o ano. 

> No duckDB isso n√£o acontece

O duckDB desmonta a tabela e guarda cada coluna em um lugar separado do arquivo. Por ex:

> Coluna ID [1, 2, 3, 4 ...]
> Coluna T√≠tulo: ["DOI-Numero-Etc-2023" - "Pesquisa Sobre: ...", ...]
> Coluna Ano: [2023, 2024, 2025]
> Coluna Fonte: ["Bluesky", "wikipedia", ...]

E para conectar as pontas ele usa o Index. O DuckDB sabe que o primeiro item da coluna ano corresponde ao primeiro item da coluna T√≠tulo. 

> Pseudoc√≥digo com a query SELECT Titulo FROM artigos WHERE Ano = 2023

1. O DuckDB escaneia o arquivo da coluna ano. Ele carrega o vetor de n√∫meros ( [ 2000, ... 2023, 2024, 2025]) e aplica um filtro sobre onde h√°, neste exemplo, 2023. A resposta do filtro √© **0 e 2**. 

2. Agora o DuckDB sabe que precisa das posi√ß√µes 0 e 2. Ele vai apenas no arquivo da coluna t√≠tulo. Pula as posi√ß√µes 1 e 3, l√™ o que interessa e d√° a resposta.

Note: Para filtrar por ano, com os nossos arquivos em gigabytes, o POSTGRES gastaria muito em texto desnecess√°rio para chegar na coluna ano. 


## Estrututra de Dados e comunica√ß√£o entre arquivos

### database.py: Arquitetura dos dados

O dabase.py √© um virtual data lake. Ele caminha pela pasta data/, acha os arquivos .parquet e diz pro DuckDB quais precisamos tratar como tabela SQL sem carregar na mem√≥ria. 

Nas linhas 67 at√© a 85 usamos glob. Se mais arquivos parquet forem adicionados (como Alysson sugeriu de mais fontes), pode demorar levar ainda mais tempo para a inicializar a API. 

### queries.py 

Aqui temos todas as queries j√° estruturadas pelo Dr. Alysson. Adicionamos "tr√™s categorias de peso" para explicar sobre a velocidade das consultas. 

### Categoria A: Dashboard Incial

Queries leem apenas o arquivo de eventos
- Fun√ß√µes: all_sources, all_events_years, all_sources_filter_years.
- Performance: < 50ms entre todas.

Essas m√©tricas de vis√£o geral l√™em direto do disco semp precisar de pr√© processamento. 

### Categoria B: Os Joins Pesados (previamente estrturados)

As queries elaboradas pelo Alysson no BigQuery entram aqui. S√£o queries complexaas com os metadados gigantescos do OpenAlex. 

- Fun√ß√µes: event_journals, fields_events.
- A l√≥gica do Join: 
> ON LOWER(SUBSTRING(a.id FROM 17)) = LOWER(b.doi)

**O problema √©** que o evento vem como "https://doi.org/10.1234/x", o OpenAlex s√≥ 10.12345. 
**A solu√ß√£o que usamos** O DuckDB corta os primeiros 16 caracteres da URL em tempo de execu√ß√£o para bater com o DOI.
**Trade-Off**: Gasta bem mais CPU, mas √© um valor irris√≥rio se comparado a velocidade em que √© executado.

**Filtro de Qualidade**

> WHERE c.score >= 0.95

N√£o mostramos qualquer classifica√ß√£o. S√≥√°reas do conhecimento onde o algoritmo tem 95% ou mais de confian√ßa.

### Categoria C: Exporta√ß√£o (WIP)

24/11/2025:

Modificamos o endpoint GET /all_events_data_filter_years_enriched/{ya}{yb} para retornar arquivo CSV direto para download. Anteriormente, o endpoint retornava JSON em formato colunar que precisava ser processado no cliente. Para concretizar a atualiza√ß√£o, alteramos as chamadas no arquivo que centraliza as queries: adicionamos a fun√ß√£o generate_csv_streaming() em queries.py (428-469). 

Tamb√©m adicionamos na main.py o import do StreamingResponse, o Endpoint agora retorna StreamingResponse com media_type="text/csv" + header Content-Disposition, que faz o download autom√°tico. **Para isso, alteramos o uso do bot√£o no frontend de JSON MAP para href simples.

Com isso, a exporta√ß√£o traz as colunas DOI, Timestamp, Year, Source, Prefix, Title, Publication year, Journal, Field.

Resumo:
  Tabelas consultadas: 7 (eventos + works + locations + sources + topics + fields)
  Dados escaneados: ~1.75 GB por request (Parquet)
  Tempo de processamento: 800ms - 1.5s (dependendo do range de anos)
  Rate limit: 10 requests/min (protege servidor)
  Uso de CPU: ~20% do tempo (com rate limit ativo)

  Trade-off: Query pesada no servidor, mas browser n√£o trava e funciona em qualquer dispositivo.

- Fun√ß√£o: all_events_data_filter_years_enriched

Essa √© a fun√ß√£o que estamos trabalhando. Por ser um pouco mais cr√≠tica, ainda √© preciso cautela na implementa√ß√£o. Ela faz 6 Left Joins que varrem todas as tabelas. A exporta√ß√£o √© completa e a opera√ß√£o mais custosa do sistema porque enriquecemos cada evento com todos os metadados dispon√≠veis. Por esse mesmo motivo, colocamos um rate limit mais restritivo, mas os dados ainda est√£o indispon√≠veis para finalizarmos a implementa√ß√£o com data streaming para cria√ß√£o do csv -> data streaming √© essencial porque, por conta do volume dos dados, esse processo se realizado no cliente pode quebrar o navegador. 

Categoria D: Busca por doi: search_dois

- Fun√ß√£o search_dois (n12)

Ao inv√©s de fazer um SQL maluco que retorna um JSON aninhado, optei por buscar os dados brutos no banco e montar o dicion√°rio/JSON no python de maneira segura.

> placeholders = ', '.join(['?' for _ in normalized_dois])

**Garanti que todos os par√¢metros usassem "?" para impedir qualquer tipo de SQL Injection via API**.
